# Challenge 1 — Weather ↔ Questions: EDA by Omar Bensatal

This folder contains the exploratory analysis for Challenge 1 (linking platform activity / questions to time and user signals). The primary artifact is the notebook `challenge_1_omar_bensatal.ipynb` plus several small CSV outputs and logs.

Files
- `challenge_1_omar_bensatal.ipynb` — modular EDA notebook (small cells; run step-by-step).
- `top_power_users.csv` — exported list of top askers and answerers (generated by the notebook).
- `../../data_change_log.md` and `../../process_log.md` — repo-level logs (we appended a process entry for this EDA run). Use `logs/log_writer.py` to append consistently.

Summary of what we did
- Loaded the Producers Direct dataset and performed a staged exploratory data analysis: schema and dtype checks, missing-value summaries, language and demographic overviews, explicit date parsing, activity over time, topic counts (where present), and text-length / token summaries for questions and responses.
- Implemented a focused "power users" extraction using canonical column names (`question_user_id` and `response_user_id`) and exported the top users to `top_power_users.csv` for quick review.
- Created time features from parsed date columns (year, month, week, weekday, hour) to support seasonality and lag analyses.

Key insights so far
- Platform activity shows a pronounced concentration of question activity around 2018 (spike). Responses do not always match the same pattern — investigate whether data collection or platform behavior changed in that period.
- Language columns are present in the dataset; question/response language distributions were inspected and should be used before any text-processing step (translation, tokenization) to pick appropriate language models/stopwords.
- Question and response text lengths vary widely; median question lengths are short while a small tail includes long descriptions. This suggests lightweight classifiers (TF-IDF / small transformer) are appropriate for initial topic labeling.
- We identified top askers and top answerers (see `top_power_users.csv`). These users are candidates for human validation as community leaders.

Limitations & concerns
- Column semantics unclear: `question_sent`, `response_sent`, `question_user_created_at`, and `response_user_created_at` require clarification from Producers Direct: are these platform ingestion timestamps, user-submitted timestamps, or externally-provided event times? Their meaning affects growth/retention analyses and any inference about user behavior.
- 2018 activity spike: the strong activity cluster around 2018 could reflect a real campaign, a data export/import artifact, or a historic pilot. Before building time-series models, confirm whether 2018 data is comparable to other years.
- Topic coverage: a `question_topic`/`response_topic` column may not be consistent or complete. If missing or noisy, apply unsupervised topic extraction (BERTopic / LDA) and then validate with a human-labeled seed set.
- Text processing is currently language-agnostic and uses a simple tokenizer. For multilingual analysis we must detect language at the message level and use language-appropriate tokenization and stopword lists, or translate using a documented pipeline.
- Missing data & null tokens: multiple columns include null-like tokens and empty strings — any production cleaning must be logged in `data_change_log.md` and justified.

How to reproduce (quick)
1. Open `challenge_1_omar_bensatal.ipynb` and run the cells sequentially. Cells are intentionally small so you can run only the parts you need.
2. Inspect `top_power_users.csv` for a short list of top askers/answerers.
3. If you run data-cleaning steps, call `logs/log_writer.append_data_change(...)` before/after to create entries in `data_change_log.md` and append the process in `process_log.md`.

Next recommended steps
- Confirm semantics of the timestamp columns with Producers Direct (critical for time-series work).
- Run a focused investigation on the 2018 spike: sample records, inspect metadata, and ask the partner whether a change in collection tooling or a campaign occurred.
- Add language-aware tokenization or a translation pipeline and rerun topic extraction.
- Build simple time-series models (weekly aggregation) to test whether weather features lead topic volume changes (requires joining with a weather dataset and defining region mapping).

GenAI usage disclosure
- No generative models were used in the analysis so far. Any future translation or topic-labeling that uses generative models must be recorded in `process_log.md` and in each challenge README under "GenAI usage".

Contact / notes
- Notebook author: Omar Bensatal (analysis in this folder).
- If you provide clarifications for the timestamp semantics or additional column name corrections, I will update the notebook and README accordingly and run a validation pass.
